\documentclass{article}
\usepackage{algorithm}
\usepackage{algorithmic}
% Core packages
\usepackage{graphicx,booktabs,hyperref}
\usepackage{amsmath,amssymb,mathtools,amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[textsize=tiny]{todonotes}
\usepackage{subcaption}    
\usepackage{multirow} 
\usepackage{array}

% Conference style
\usepackage[accepted]{icml2025}

% Hyperref compatibility
\providecommand{\theHalgorithm}{\arabic{algorithm}}

% Algorithms 
\usepackage{algorithm}
\usepackage{algorithmic}     

\usepackage{natbib}

% Theorem environments
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Colors & math
\usepackage{xcolor}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor[rgb]{0,0.6,0}{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbZ}{\mathbb{Z}}


% Running title (update this)
\icmltitlerunning{Large Language Models for Optimization Solver Parameter Tuning}

\begin{document}

% Switch to single column for easier writing
\onecolumn

\icmltitle{Large Language Models for Optimization Solver Parameter Tuning}

% \begin{icmlauthorlist}
%     \icmlauthor{Zhengzhong You}{independent}
%     \icmlauthor{Xinshang Wang}{alibaba}
%     \icmlauthor{Wotao Yin}{alibaba}
% \end{icmlauthorlist}

% \icmlaffiliation{independent}{Independent Researcher}
% \icmlaffiliation{alibaba}{Alibaba Group US, Bellevue, WA}

% \icmlcorrespondingauthor{Zhengzhong You}{routeopt@outlook.com}

% Keywords
\icmlkeywords{Optimization, SCIP, LLM, Parameter Tuning}

\vskip 0.3in

% \printAffiliationsAndNotice{}

\begin{abstract}
    Optimization solvers are powerful tools capable of addressing a wide range of complex computational problems; however, their performance is highly sensitive to parameter configurations, which can vary dramatically across problem instances. Tuning these parameters is inherently difficult due to the vast, high-dimensional, and often non-convex search space, as well as the intricate interactions among solver settings that influence convergence behavior and efficiency. Existing tuning strategies can be broadly categorized into two types. The first category comprises automated tuning methods, such as \red{Bayesian optimization and evolutionary search}, which can effectively explore parameter spaces for specific problem classes but often suffer from high computational costs, poor generalization, and limited interpretability. The second category involves manual tuning guided by human experts, who analyze solver logs and heuristically identify promising parameter sets. Although this approach typically yields superior performance, it demands substantial expert effort, making it impractical for large-scale or frequently changing problem settings. \red{Motivated by these challenges, we propose a Large Language Model (LLM)-based framework for solver parameter tuning, which leverages the reasoning and contextual understanding capabilities of LLMs to efficiently generate, refine, and adapt parameter configurations across diverse optimization tasks.}

    % this is the tool purpose
    % This paper proposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that leverages both Large Language Models (LLMs) and Evolutionary Computation (EC) methods for Automatic Heuristic Design (AHD). 

    % this is the overview of what ur doing, basically explanation of the method
    % EoH represents the ideas of heuristics in natural language, termed thoughts. They are then translated into executable codes by LLMs. The evolution of both thoughts and codes in an evolutionary search framework makes it very effective and efficient for generating high-performance heuristics. 


    % this is numerical
    % Experiments on three widely studied combinatorial optimization benchmark problems demonstrate that EoH outperforms commonly used handcrafted heuristics and other recent AHD methods including FunSearch. Particularly, the heuristic produced by EoH with a low computational budget (in terms of the number of queries to LLMs) significantly outperforms widely-used human hand-crafted baseline algorithms for the online bin packing problem.

\end{abstract}

\section{Introduction}


Optimization problems are pervasive across numerous domains, ranging from traditional areas such as logistics, manufacturing, and finance to emerging fields including machine learning, data science, and artificial intelligence. \blue{need cite} The ability to effectively solve these problems heavily relies on optimization solvers. Over the past few decades, a wide range of solvers—such as Gurobi (Gurobi Optimization, 2024), CPLEX (IBM, 2024), MindOpt, and SCIP (Achterberg, 2009)—have been developed to handle diverse optimization formulations, including linear programming (LP), mixed-integer programming (MIP), quadratic programming (QP), and constraint satisfaction problems (CSP). These solvers integrate sophisticated algorithms and heuristics that efficiently navigate the solution space to obtain optimal or near-optimal solutions, thereby enabling their application in a broad array of real-world contexts.

Despite their effectiveness, the performance of optimization solvers is highly sensitive to parameter configurations. Different parameter settings can lead to substantial differences in convergence speed, solution quality, and computational efficiency. For instance, in MIP solvers such as SCIP, parameters governing node selection strategies, cutting-plane generation, and primal heuristics can profoundly affect the solver’s performance across problem instances (Achterberg, 2009). Consequently, identifying appropriate parameter configurations is essential for achieving optimal solver efficiency. \blue{cite the number that how these parameters showing numbers difference.}

However, parameter tuning poses a formidable challenge for several reasons. First, the parameter space is typically vast and high-dimensional, rendering exhaustive search infeasible. For example, SCIP offers more than two thousand tunable parameters, making individual adjustment impossible, particularly given the complex interactions among them. Second, the mapping between parameter settings and solver performance is highly nonlinear and intricate, with strong dependencies among parameters. Third, optimal configurations often vary widely across different problem instances or categories, implying that a universally effective parameter setting does not exist and necessitating adaptive tuning strategies.

To address these challenges, various parameter tuning approaches have been explored in the literature, which can be broadly categorized into two groups: automated tuning and manual tuning by human experts.


\green{Automated methods—such as Bayesian optimization (Snoek et al., 2012; Hutter et al., 2011) and evolutionary search (Eiben \& Smith, 2015; Jansen \& Zarges, 2020)—systematically explore the parameter space to identify effective configurations for specific problem classes. Although these methods can achieve satisfactory performance in certain contexts, they often entail substantial computational costs, exhibit limited generalization to unseen instances, and yield configurations that are difficult to interpret.}


\section{Background and Related Work}

Solver parameter tuning is a specific instance of algorithm configuration (AC; \citealt{Hutter2009ParamILS}), which seeks high-performing parameter settings for a given algorithm over a set of problem instances. Early work emphasized large, homogeneous instance sets and optimized configurations under a limited evaluation budget, with generalization assessed on separate test instances; this is important because default settings are often suboptimal for specific instance distributions, while tailored configurations can yield substantial gains, e.g., up to $52\times$ test-time speedups for CPLEX on industrial MIP \citep{Hutter2010CPAIOR}. Methods include model-free heuristics such as ParamILS \citep{Hutter2009ParamILS} and irace \citep{LopezIbanez2016irace}, and model-based strategies such as SMAC \citep{Hutter2011SMAC} and RBFOpt \citep{Costa2018RBFOpt}, which use surrogate models to guide the search. When the target is a single instance (a singleton set), a configuration optimized for overall performance can be suboptimal \citep{Li2025BenLOC}. Instance-Specific Algorithm Configuration (ISAC; \citealt{Kadioglu2010ISAC}) addresses this by predicting parameters \emph{before} they take effect, e.g., setting branching-related options prior to branching \citep{Li2025BenLOC}. A classical ISAC approach clusters instances and assigns per-cluster configurations \citep{Kadioglu2010ISAC}. More recent work leverages instance features, static (e.g., numbers of variables and constraints, constraint-matrix density) and/or early-stage dynamic signals (e.g., presolve statistics), with supervised or unsupervised models to select configurations for unseen instances, demonstrating improvements on MIP \citep{Song2023ISACDeep,Liu2024L2PMIP,Li2025BenLOC}.


Large language model (LLM) methods increasingly influence scientific computing and optimization along two main axes: (i) modeling assistance and text-to-optimization, where LLMs translate natural-language problem statements \citep{ramamonjison2023nl4opt} or solver logs into algebraic formulations and executable code \citep{huang2025orlm}; and (ii) optimization guidance, where LLMs propose heuristics \citep{huang2024words,iklassov2024self}, repair schedules \citep{abgaryan2024llms}, or structured edits that a solver subsequently validates \citep{hao2024large}. These advances succeed primarily due to the domain knowledge encoded within LLMs—a property shared by our LLM agent, \emph{SolverMind}. Building on this foundation, SolverMind acts as an optimization expert for algorithm configuration (AC): it interprets solver logs to generate structured, constraint-consistent parameter edits that substantially shrink the effective search space, while remaining compatible with strong AC baselines such as ParamILS and SMAC for fair benchmarking. SolverMind can process logs from a batch of instances to recommend a configuration that maximizes aggregate performance on a static test set (classical AC) or, alternatively, determine parameters for a single instance (ISAC). The latter setting also produces labels (ground-truth values) for supervised ISAC, thereby reducing the need for enumeration-based search during label generation and making the creation of training data more efficient.

\section{Problem Formulation}

Let $\mathcal{D}$ denote an unknown distribution over mixed-integer problem instances and let $\mathcal{I}\subset\mathrm{supp}(\mathcal{D})$ be a finite subset when needed for evaluation. Let $S$ be a MIP solver and $E$ the execution environment (hardware, software versions, thread policy). Fix a per-run time budget $\tau>0$, and define the parameter space $\mathcal{P}=\prod_{j=1}^d \mathcal{P}_j$, where each $\mathcal{P}_j\subseteq\mathbb{R}$, $\mathbb{Z}$, or a finite alphabet. For configuration $p\in\mathcal{P}$ and instance $i$, a budgeted run returns
\[
\Phi(p;i,E,\tau)=\big(t(p,i),\, s(p,i),\, z^{\mathrm{pr}}(p,i),\, z^{\mathrm{du}}(p,i),\, n(p,i)\big),
\]
with wall-clock time $t$, status $s\in\{\text{opt},\text{inf},\text{unb},\text{tl (time limit)},\text{other}\}$, primal and dual bounds $z^{\mathrm{pr}},z^{\mathrm{du}}$ when defined, and processed nodes $n$. The optimality gap is
\[
g(p,i) =
\begin{cases}
\displaystyle
\frac{|z^{\mathrm{pr}} - z^{\mathrm{du}}|}{\max(\varepsilon, \min(|z^{\mathrm{pr}}|, |z^{\mathrm{du}}|))}, & \text{if } z^{\mathrm{pr}} \text{ and } z^{\mathrm{du}} \text{ have the same sign}, \\[6pt]
+\infty, & \text{if } z^{\mathrm{pr}} \text{ and } z^{\mathrm{du}} \text{ have opposite signs},
\end{cases}
\]
and runs exceeding $\tau$ are truncated at $\tau$ and marked $\text{tl}$. To reflect practical priorities, define
\[
\mathrm{tier}(s)=
\begin{cases}
0, & s\in\{\text{opt},\text{inf},\text{unb}\},\\
1, & \text{otherwise}.
\end{cases}
\]
We aggregate per-instance performance via a strictly increasing scalarization
\begin{equation}\label{eq_sigma_obj}
    \sigma(p,i)=f_1\!\big(\mathrm{tier}(s(p,i))\big)+f_2\!\big(t(p,i)/\tau\big)+f_3\!\big(g(p,i)\big)+f_4\!\big(n(p,i)/N_{\max}\big),
\end{equation}
where $N_{\max}>0$ normalizes node counts and each $f_k:\mathbb{R}_{\ge 0}\to\mathbb{R}_{\ge 0}$ is monotonically increasing. We assume $\sigma(p,i)>0$; otherwise we replace $\sigma$ by $\sigma+\varepsilon$ with a fixed $\varepsilon\in(0,10^{-9}]$ for geometric means.

\paragraph{Population objective (distribution-level).}
The goal of algorithm configuration is to choose a single configuration that performs well \emph{on the distribution}, not only on the few instances observed. Using the geometric-mean aggregator, the population risk is
\begin{equation}
\label{eq:pop}
J_{\mathcal{D}}(p)\;=\;\exp\!\Big(\,\mathbb{E}_{i\sim\mathcal{D}}\big[\log \sigma(p,i)\big]\,\Big).
\end{equation}
The ideal configuration solves the stochastic black-box program
\begin{equation}
\label{eq:stochastic_config}
\min_{p\in\mathcal{P}}\; J_{\mathcal{D}}(p)
\quad\text{with evaluations defined by budgeted runs at time }\tau.
\end{equation}

\paragraph{Empirical objective (sample-limited tuning).}
In practice we can access only a small random sample $S=\{i_1,\dots,i_M\}$ drawn i.i.d.\ from $\mathcal{D}$ (with $M$ constrained by cost and privacy). We therefore optimize the empirical geometric mean
\begin{equation}
\label{eq_configuration}
\widehat{J}_S(p)\;=\;\exp\!\Big(\frac{1}{M}\sum_{i\in S}\log \sigma(p,i)\Big),
\end{equation}
and select
\begin{equation}
\label{eq:emp_opt}
\hat{p}\;\in\;\arg\min_{p\in\mathcal{P}}\;\widehat{J}_S(p).
\end{equation}
Generalization is then assessed on a disjoint test set $T\subset\mathrm{supp}(\mathcal{D})$ by reporting $\widehat{J}_T(\hat{p})$ and related metrics. When $|S|=1$, the formulation reduces to the instance-specific configuration (ISAC) setting.

\paragraph{Remarks.}
Problems \eqref{eq:stochastic_config}–\eqref{eq:emp_opt} are black-box, constrained, mixed discrete–continuous programs without closed-form objectives; each evaluation requires running the solver under budget $\tau$ and is noisy across platforms. The use of the logarithm in \eqref{eq:pop}–\eqref{eq_configuration} is the standard device linking geometric means to additive population risk, ensuring that minimizing $\widehat{J}_S$ targets the distribution-level objective $J_{\mathcal{D}}$ rather than overfitting the specific sample.



\section{LLM-Guided Tuning Agent: SolverMind}

We propose a closed-loop tuning agent, \textit{SolverMind}, that minimizes the \emph{distributional} objective via its sample-based surrogate: given an i.i.d.\ training batch \(S=\{i_1,\dots,i_M\}\) drawn from \(\mathcal{D}\), the agent iteratively proposes configurations and selects the incumbent by the empirical risk \(\widehat{J}_S(\cdot)\) in \eqref{eq_configuration} (reducing to ISAC when \(M=1\)). Let \(\mathcal{W}\subseteq\{1,\dots,d\}\) denote admissible parameter indices with domain constraints, \(m\in\mathbb{Z}_{\ge1}\) the maximum number of edits per trial, and \(K\in\mathbb{Z}_{\ge1}\) the trial budget. The agent maintains a history
\[
H_k=\big\{\,\big(p^{(j)},\{\Phi(p^{(j)};i,E,\tau)\}_{i\in S},\widehat{J}_S(p^{(j)})\big):\, j<k\,\big\}.
\]

\begin{algorithm}[H]
    \caption{LLM-Guided Solver Configuration (empirical objective over \(S\))}
    \begin{algorithmic}[1]\label{algo_llm}
        \REQUIRE solver \(S\), environment \(E\), time budget \(\tau\), whitelist \(\mathcal{W}\), edit cap \(m\), trial cap \(K\), training set \(S\)
        \STATE Extract batch features \(x_S \gets \{F(i): i\in S\}\) (e.g., counts, densities, statistics).
        \STATE Initialize \(p^{(0)}\) to solver defaults; for all \(i\in S\) evaluate \(\Phi(p^{(0)};i,E,\tau)\); compute \(\widehat{J}_S(p^{(0)})\); set incumbent \(p^\star\gets p^{(0)}\).
        \FOR{$k=1,2,\dots,K$}
            \STATE Build prompt \(\Pi_k=\Pi(x_S,H_k,\mathcal{W},m)\) with schema constraints for edits.
            \STATE Query LLM for candidate edits \(\Delta p^{(k)}\) (restricted to \(\mathcal{W}\)); apply up to \(m\) valid changes to obtain \(p^{(k)}\).
            \STATE Validate \(p^{(k)}\) (whitelist membership, domain admissibility, novelty vs.\ prior \(p^{(j)}\)); \IF{invalid} \STATE repair or resample \ENDIF
            \STATE For all \(i\in S\), evaluate \(\Phi(p^{(k)};i,E,\tau)\); compute \(\widehat{J}_S(p^{(k)})=\exp\!\big(\tfrac{1}{M}\sum_{i\in S}\log\sigma(p^{(k)},i)\big)\); append \(\big(p^{(k)},\{\Phi(\cdot)\}_{i\in S},\widehat{J}_S(p^{(k)})\big)\) to \(H_{k+1}\).
            \IF{$\widehat{J}_S(p^{(k)})<\widehat{J}_S(p^\star)$}
                \STATE \(p^\star \gets p^{(k)}\)
            \ENDIF
            \STATE Convergence check: query LLM with a saturation prompt using the trajectory \(\{\widehat{J}_S(p^{(j)})\}_{j\le k}\).
            \IF{predicted marginal gain \(<\epsilon\) for \(\ell\) consecutive trials}
                \STATE \textbf{break}
            \ENDIF
        \ENDFOR
        \ENSURE incumbent \(p^\star\) minimizing \(\widehat{J}_S(p)\) on \(S\).
    \end{algorithmic}
\end{algorithm}

The prompt \(\Pi_k\) aggregates (i) batch features \(x_S\); (ii) concise history \(H_k\) with prior configurations and \(\widehat{J}_S\) values; (iii) curated log excerpts (presolve progress, node growth, cut/propagation efficacy, memory pressure) summarized across \(S\); and (iv) a reinforced whitelist that restricts edits to core parameters and meta-settings. The validator enforces whitelist and domain admissibility and the \(m\)-edit cap, rejecting redundant or invalid proposals. The saturation test implements an LLM-guided early stopping rule that halts exploration when additional trials are unlikely to reduce \(\widehat{J}_S\) materially. The final configuration \(p^\star\) is subsequently evaluated on a disjoint test set \(T\) via \(\widehat{J}_T(p^\star)\) to estimate distribution-level generalization toward \(J_{\mathcal{D}}(\cdot)\) in \eqref{eq:pop}.


\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Settings}


\paragraph{Benchmarks and datasets.}
We evaluate on [\textbf{fill}] structured, real-world application families and one synthetic distributional suite. For each family, we apply a controlled train–test split defined as  
\begin{equation}\label{eq_train}
L = \min\!\big\{\lfloor 0.3\,|\mathcal{I}|\rfloor,\, 20\big\},
\end{equation}
where \(L\) training instances are sampled i.i.d.\ from each family, and the remaining instances constitute the test set. Restricting the training subset to a small fraction of the available instances reflects realistic computational and data-governance constraints commonly encountered in industrial optimization, where obtaining or repeatedly solving large instance collections is impractical.  

The performance of SolverMind on the training set reflects its ability to efficiently solve the configuration optimization problem~\eqref{eq_configuration}. The test set, by contrast, evaluates generalization—namely, whether a configuration learned from a limited number of training instances can improve solver performance on previously unseen instances. This design follows the principle of classical algorithm configuration (AC), whose objective is to identify parameter settings that remain robust across related but distinct problem instances.  

For the instance-specific algorithm configuration (ISAC) setting, evaluation is performed exclusively on the training instances. In this context, the goal is not cross-instance generalization but the ability to identify near-optimal configurations for individual instances with minimal solver invocations. Accordingly, ISAC experiments benchmark SolverMind against established optimization methods for solving problem~\eqref{eq_configuration} on a single instance, comparing both the achieved objective values and the number of solver runs required.


\begin{itemize}

\item \textit{Capacitated Vehicle Routing Problem (CVRP).}  
The objective is to find a set of minimum-cost vehicle routes that start and end at a depot, ensuring each customer is visited exactly once and that the total demand served on any route does not exceed the vehicle capacity \citep{dantzig1959truck}. We adopt the two-commodity network-flow formulation of \citet{baldacci2004exact}, a compact mixed-integer formulation that achieves optimality without requiring column or row generation. The model is given by

\begin{equation}
\begin{aligned}
\min \quad & \sum_{\{i,j\}\in \mathcal{E}} c_{ij}\,x_{ij} \\
\text{s.t.} \quad 
& \sum_{j\in \mathcal{V}} \big(y_{ji} - y_{ij}\big) = 2d_i, && \forall i\in \mathcal{C}, \\
& \sum_{j\in \mathcal{C}} y_{0j} = d(\mathcal{C}), \\
& \sum_{j\in \mathcal{C}} y_{j0} = K Q - d(\mathcal{C}), \\
& \sum_{j\in \mathcal{C}} y_{n+1,j} = K Q, \\
& y_{ij} + y_{ji} = Q\,x_{ij}, && \forall \{i,j\}\in \mathcal{E}, \\
& \sum_{j\in \mathcal{V},\, j>i} x_{ij} + \sum_{j\in \mathcal{V},\, j<i} x_{ji} = 2, && \forall i\in \mathcal{C}, \\
& y_{ij} \ge 0, \; y_{ji} \ge 0, && \forall \{i,j\}\in \mathcal{E}, \\
& x_{ij} \in \{0,1\}, && \forall \{i,j\}\in \mathcal{E},
\end{aligned}
\end{equation}
where \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) is a complete undirected graph with two depot copies, \(\mathcal{V}=\{0,1,\dots,n,n+1\}\), and customer set \(\mathcal{C}=\mathcal{V}\setminus\{0,n+1\}\). The edge set is
$\mathcal{E}=\big\{\{i,j\}: i,j\in\mathcal{V}\setminus \{n+1\},\ i<j\big\}\cup\big\{\{i,n+1\}, i\in \mathcal C\big\}$. The travel cost on edge \(\{i,j\}\) is \(c_{ij}=c_{ji}\ge 0\). Each customer \(i\in\mathcal{C}\) has demand \(d_i\ge 0\) and \(d(\mathcal{C})=\sum_{i\in\mathcal{C}} d_i\). Vehicles have capacity \(Q>0\), and \(K\in\mathbb{Z}_{\ge 0}\) denotes the fleet size. Binary variables \(x_{ij}\) (for \(\{i,j\}\in\mathcal{E}\)) indicate whether edge \(\{i,j\}\) is used, while continuous variables \(y_{ij},y_{ji}\ge 0\) represent the two commodities (vehicle load and residual capacity) flowing on edge \(\{i,j\}\).


We use problem instances from the \texttt{XML100} series of CVRPLIB~\citep{cvrplib}, a structured benchmark suite widely used in learning-based CVRP research. The XML100 collection consists of 10,000 instances systematically generated as the Cartesian product of four categorical design factors: depot location types (\(w\in\{1,2,3\}\)), customer spatial distributions (\(x\in\{1,2,3\}\)), demand profiles (\(y\in\{1,\dots,7\}\)), and average route length classes (\(z\in\{1,\dots,6\}\)). Each unique combination defines a subgroup labeled \texttt{XML100\_wxyz}. In this study, we focus on the subgroup \texttt{XML100\_3346}, which contains 26 instances, each with 100 customers. Because solving such large instances to optimality within a [\textbf{fill}]-hour time limit is computationally challenging, we select the first [\textbf{fill}] customers from each instance to construct reduced problems for our experiments. Following the sampling rule in Eq.~\eqref{eq_train}, we allocate 7 instances for training and the remaining 19 for testing.
\end{itemize}



\paragraph{Solver, parameters, and whitelists.}
Primary experiments use \texttt{SCIP~9.2.4} \citep{achterberg2009scip, bolusani2024scip}. Each solver is paired with a curated whitelist \(\mathcal{W}\) of admissible parameters. Edit caps are \(m\in\{\!1,3,5\!\}\), and trial caps are \(K\in\{\!8,16,32,64\!\}\). We evaluate three whitelist regimes:

\begin{itemize}
\item \emph{Full}: the complete set of tunable parameters documented in the solver manual \citep{scipparams924}.

\item \emph{Curated}: a mid-size whitelist encompassing the primary solver control knobs, with the following representative parameters prioritized for tuning:
\begin{itemize}
  \item \textbf{Branching} — \texttt{branching/} \\
        \texttt{scorefunc} \\
        \texttt{preferbinary} \\
        \texttt{relpscost/\{minreliable,maxreliable,sbiterquot,sbiterofs\}}
  \item \textbf{Node selection} — \texttt{nodeselection/} \\
        \texttt{\{bestestimate,dfs,bfs\}/stdpriority} \\
        \texttt{childsel}
  \item \textbf{Cutting planes} — \texttt{separating/} \\
        \texttt{\{maxrounds,maxroundsroot,maxcuts,maxcutsroot\}} \\
        \texttt{\{gomory,mir,cmir,flowcover,clique,knapsackcover,oddcycle\}\\/\{freq,maxrounds,maxroundsroot\}}
  \item \textbf{Presolve} — \texttt{presolving/} \\
        \texttt{\{maxrounds,maxrestarts,abortfac\}} \\
        \texttt{\{probing,aggregation,boundshift,dualfix,implications,trivial\}/maxrounds}
  \item \textbf{Primal heuristics} — \texttt{heuristics/} \\
  \texttt{feaspump/\{freq,freqofs,maxdepth,maxlpiterquot,maxlpiterofs,beforecuts\}} \\
  \texttt{rins/\{nodesofs,nodesquot,minnodes,maxnodes,nwaitingnodes,minfixingrate\}} \\
  \texttt{localbranching/\{neighborhoodsize,nodesofs,nodesquot,lplimfac\}} \\
  \texttt{rens/\{nodesofs,nodesquot,minnodes,maxnodes,minfixingrate,startsol\}}%
  \item \textbf{Tolerances} — \texttt{numerics/} \\
        \texttt{\{feastol,epsilon,dualfeastol\}}
\end{itemize}

\item \emph{Minimal}: a highly focused subset containing two or three high-impact parameters per category:
\begin{itemize}
    \item \textbf{Branching} — \texttt{branching/scorefunc} 
    \item \textbf{Node selection} — \texttt{nodeselection/\{bestestimate,dfs\}/stdpriority}
    \item \textbf{Cutting planes} — \texttt{separating/\{maxroundsroot,maxcutsroot\}}
    \item \textbf{Presolve} — \texttt{presolving/\{maxrounds,abortfac\}}
        \item \textbf{Primal heuristics} — \texttt{heuristics/feaspump/\{freq,maxlpiterquot\}}
\end{itemize}
\end{itemize}

% To quantify the search space under each regime and document the exact composition by parameter type, Table~\ref{tab:param-counts} reports totals and type-wise counts for our \texttt{SCIP~9.2.4} build. The curated whitelist reduces the full space (2{,}979 tunables) to 35 parameters while retaining the levers most predictive of performance (branching reliability, node selection priorities, root cut intensity, presolve rounds, and heuristic budgets). We treat \texttt{heuristics/emphasis} as a meta-setting and do not include it in the counts; candidate value grids for each regime are provided in the artifact and appendix for reproducibility.

% \begin{table}[htp]
% \centering
% \small
% \caption{\textbf{SCIP\,9.2.4 parameter inventory by regime.} Counts by type (Boolean, Integer, Continuous, Categorical). “Meta” lists non-standard meta-presets (not counted in totals). Full-set counts are exact from our build; Curated and Minimal follow the whitelists used in our experiments.}
% \label{tab:param-counts}
% \begin{tabular}{lrrrrrr}
% \hline
% \textbf{Regime} & \textbf{Total} & \textbf{Boolean} & \textbf{Integer} & \textbf{Continuous} & \textbf{Categorical} & \textbf{Meta} \\
% \hline
% Full (all tunables) & 2{,}979 & 931 (31.3\%) & 1{,}303 (43.7\%) & 668 (22.4\%) & 77 (2.6\%) & -- \\
% Curated (mid-size)  & 35       & 1            & 24              & 8               & 2               & 1 \\
% Minimal (targeted)  & 7        & 0            & 5               & 1               & 1               & 1 \\
% \hline
% \end{tabular}

% \vspace{4pt}
% \raggedright
% \emph{Notes.} Meta \(=\) \texttt{heuristics/emphasis} preset. The Minimal row reports totals excluding this meta-setting.
% \end{table}


To quantify the search space under each regime and document the exact composition by parameter type, Table~\ref{tab:param-counts} reports totals and type-wise counts for our \texttt{SCIP~9.2.4} build. The curated whitelist reduces the full space (2{,}979 tunables) to 69 parameters while preserving the primary levers that drive performance—branching reliability, node-selection priorities, root cut intensity, presolve rounds, heuristic budgets, and numerical tolerances. Candidate value grids for each regime are provided in the artifact and appendix for reproducibility.

\begin{table}[htp]
\centering
\small
\caption{\textbf{SCIP\,9.2.4 parameter inventory by regime.} Counts by type (Boolean, Integer, Continuous, Categorical). Full-set counts are exact from our build; Curated and Minimal follow the updated whitelists used in our experiments.}
\label{tab:param-counts}
\begin{tabular}{lrrrrr}
\hline
\textbf{Regime} & \textbf{Total} & \textbf{Boolean} & \textbf{Integer} & \textbf{Continuous} & \textbf{Categorical} \\
\hline
Full (all tunables) & 2{,}979 & 931 (31.3\%) & 1{,}303 (43.7\%) & 668 (22.4\%) & 77 (2.6\%) \\
Curated (mid-size)  & 69       & 2            & 52              & 12              & 3               \\
Minimal (targeted)  & 9        & 0            & 6               & 2               & 1               \\
\hline
\end{tabular}

\vspace{4pt}
\raggedright
\emph{Notes.} Curated and Minimal counts are recomputed from the updated whitelist; candidate value grids for each regime are provided in the artifact and appendix for reproducibility.
\end{table}



\paragraph{Base LLMs.}
Our experiments require general-purpose language models that (i) ingest long solver logs and structured features, (ii) support constrained, tool-oriented outputs (JSON/function calling) for parameter edits, and (iii) offer reproducible decoding controls (temperature, top-$p$, seed) and manageable latency–cost trade-offs. We therefore consider contemporary flagship and open-weight models with strong reasoning and tool-use capabilities: OpenAI GPT-5 for primary results \citep{openai2025gpt5}, Anthropic Claude~3.5~Sonnet as a high-accuracy closed model \citep{anthropic2024claude35}, Google Gemini~2.5~Pro for long-context code/analysis tasks \citep{google2025gemini25pro}, Meta Llama~3.1~405B as an open-weight baseline amenable to on-prem deployment \citep{meta2024llama31}, Mistral Large~2 as a compact high-performance alternative with strong function calling \citep{mistral2024large2}, DeepSeek-V3 as a cost-efficient high-throughput option \citep{deepseek2024v3}, and Cohere Command (A/R$+$) for RAG-centric workflows \citep{cohere2025command}. Unless noted, we enable each model’s function-calling or tool-use interface, request schema-validated JSON for edits, fix decoding hyperparameters across runs, and normalize context limits to ensure identical prompt material per method.


\paragraph{Baselines for optimizing \(\widehat{J}_S(p)\).}
We compare SolverMind against two state-of-the-art black-box optimizers designed for expensive, mixed discrete–continuous search spaces. Both baselines directly minimize the empirical geometric-mean objective in \eqref{eq_configuration} by optimizing the equivalent additive surrogate \(L(p)=\tfrac{1}{M}\sum_{i\in S}\log\sigma(p,i)\), given the monotonicity of the exponential transformation. Integer parameters are handled via rounding, and categorical parameters are encoded using one-hot or label representations.

\emph{SMAC (Sequential Model-based Algorithm Configuration).} SMAC is a Bayesian optimization framework specifically developed for algorithm configuration. It models \(L(p)\) with an ensemble of randomized regression trees (extremely randomized forests) to capture nonstationary and heteroscedastic response surfaces in mixed domains and selects candidates using Monte Carlo Expected Improvement (EI) under the model’s predictive distribution \citep{Hutter2011SMAC}. Classical SMAC implementations employ intensification and adaptive capping: candidate configurations are incrementally evaluated on subsets of \(S\) and truncated once statistically dominated. For fairness in our comparison, we disable these and measure only the number of complete evaluations performed, running all configurations on the full instance set \(S\) under identical time budgets.

\emph{RBFOpt (Radial Basis Function Optimization).} RBFOpt constructs a global radial-basis surrogate model of \(L(p)\) over the mixed discrete–continuous domain and alternates between global exploration and local trust-region refinement around the incumbent solution \citep{Costa2018RBFOpt}. It naturally handles bounded, integer, and categorical parameters through hybrid encodings and employs deterministic acquisition rules that balance predicted improvement with model reliability. RBFOpt is recognized as a competitive derivative-free optimizer for expensive black-box tasks, including mixed-integer settings common in solver tuning, and serves as a strong benchmark in algorithm-configuration studies.

All baselines are evaluated under identical computational budgets, adhering to the same per-run time limits \(\tau\), parameter domain \(\mathcal{P}\), and training set \(S\). Detailed implementation settings—including parameter encodings, random seeds, and early-stopping criteria—are provided in the supplementary material to ensure reproducibility.



\paragraph{Choice of \(f_1\)–\(f_4\).}

The overall performance score  \eqref{eq_sigma_obj}
aggregates distinct aspects of solver behavior. The relative scaling and functional form of the penalty functions \(f_1\)–\(f_4\) determine which performance dimensions dominate the optimization objective. Properly selecting these functions ensures that \(\sigma(p,i)\) faithfully reflects practical solver priorities rather than overemphasizing a single component.

\emph{Extreme weight limits.}
To illustrate the trade-offs among objectives, consider four limiting regimes:


\begin{itemize}
    \item When \(\displaystyle \lim_{x\to\infty} \frac{f_1(x)}{\max_{k\neq1} f_k(x)} = \infty\), the score depends almost exclusively on termination status. In this regime, configurations that achieve any conclusive result (\texttt{opt}, \texttt{inf}, or \texttt{unb}) are strongly preferred, regardless of runtime, gap, or resource usage.
    \item Conversely, if \(\displaystyle \lim_{x\to\infty} \frac{f_2(x)}{\max_{k\neq2} f_k(x)} = \infty\), the objective effectively becomes a pure runtime minimization criterion, disregarding solution quality and termination outcomes.
    \item When \(\displaystyle \lim_{x\to\infty} \frac{f_3(x)}{\max_{k\neq3} f_k(x)} = \infty\), the metric focuses exclusively on closing the optimality gap, often leading to configurations that spend excessive time pursuing marginal primal improvements when the problem instance is too challenging to solve.
    \item Similarly, if \(\displaystyle \lim_{x\to\infty} \frac{f_4(x)}{\max_{k\neq4} f_k(x)} = \infty\), the objective primarily rewards minimizing node counts or memory consumption, which generally shows weak correlation with overall solver efficiency.
\end{itemize}


\emph{Adopted functions.} As shown empirically in Section~[\textbf{fill}], such extreme configurations yield unbalanced performance and degraded overall efficiency. To address this, we adopt a balanced formulation that weights all components proportionally to their practical significance, as summarized in Table~\ref{tab_f_functions}.

\begin{table}[htp]
\centering
\small
\caption{\textbf{Adopted parameterizations of \(f_1\)–\(f_4\)} and their underlying rationale. Each function balances solver completion, runtime efficiency, accuracy, and resource utilization.}
\label{tab_f_functions}
\begin{tabular}{l p{10cm}}
\hline
\textbf{Function} & \textbf{Rationale} \\
\hline
\(f_1(x)=\min\{10^3x,\,10^3\}\) &
A large coefficient imposes a strong penalty on runs that fail to terminate successfully within the time budget, emphasizing solver completion as the primary goal. \\[4pt]

\(f_2(x)=\min\{x,\,10^3\}\) &
A linear penalty directly proportional to normalized runtime, promoting faster convergence among feasible runs while preventing excessive emphasis on minor timing differences. \\[4pt]

\(f_3(x)=\min\{e^{x}-1,\,10^3\}\) &
An exponential penalty capturing the nonlinear difficulty of reducing optimality gaps. For large \(x\), the exponential growth reflects the sharply increasing computational effort required to achieve tighter bounds. For small \(x\), since \(e^{x}-1\approx x\), the penalty is nearly linear—an intentional design choice, as configurations yielding similarly small gaps are typically sufficient in practical applications and should not be over-penalized. \\[4pt]

\(f_4(x)=\min\{10^{-6}x,\,10^3\}\) &
A mild penalty on the normalized number of explored nodes (or memory usage), introduced to discourage excessive search expansion while keeping this aspect secondary to completion and runtime. \\
\hline
\end{tabular}
\end{table}

We impose an upper bound of \(10^3\) in each \(f_k\) to prevent any single instance with an exceptionally large score from dominating the aggregate objective \(\widehat{J}_S(p)\). Since the optimization minimizes the geometric mean of \(\sigma(p,i)\) across all instances in \(S\), this cap ensures balanced optimization behavior—emphasizing configurations that perform consistently well across the entire instance set rather than overfitting to a few extreme or pathological cases. This parameterization provides a balanced trade-off among solver reliability, runtime efficiency, optimality gap reduction, and memory utilization, empirically delivering the best overall performance across all evaluated benchmarks.


This parameterization provides a balanced trade-off among runtime efficiency, optimality gap reduction, and memory utilization, empirically delivering the best overall performance across all evaluated benchmarks.

\paragraph{Evaluation protocol.} [\textbf{fill}]

\paragraph{Compute environment and budgets.}
All runs use identical hardware and pinned software: CPU [fill], RAM [fill]\,GB, OS [fill], threads \(=\)[fill] across all methods, seeds \(\{1,\dots,5\}\). Each run has a per-instance time budget \(\tau=\)[fill]\,s; sensitivity uses \(\tau\in\{300,900,3600\}\). Container images and exact command lines are released.



\subsection{Results}\label{sec_results}

We first present the key experimental results, showing that \textit{SolverMind}, as an optimizer for the configuration problem~\eqref{eq_configuration}, consistently outperforms state-of-the-art baselines across all evaluation metrics. The experimental setup for Algorithm~\ref{algo_llm} employs a maximum edit cap of \(m=3\), a total trial limit of \(K=16\), and the whitelist regime fixed to the \emph{Curated} configuration. These choices are empirically justified in the ablation study (see Section~\textbf{[fill]}).

Tables~\ref{tab:main_results_300}–\ref{tab:main_results_3600} report results under time budgets \(\tau\in\{300,900,3600\}\,\mathrm{s}\). For each \(\tau\), we compare \textbf{Default}, \textbf{SMAC} \citep{Hutter2011SMAC}, \textbf{RBFOpt} \citep{Costa2018RBFOpt}, and \textbf{SolverMind}. We report:
\begin{itemize}
    \item \textbf{Runs}: number of solver evaluations used to produce the final configuration (for AC modes, measured on the training set; for ISAC, mean runs per instance).
    \item \(\widehat{J}_S\): empirical objective value defined in Eq.~\eqref{eq_configuration}.
    \item \textbf{Solve [\%]}: fraction of instances with conclusive status (\texttt{opt}/\texttt{inf}/\texttt{unb}).
    \item \textbf{GM-t [s]}: geometric-mean runtime over solved instances (seconds).
    \item \textbf{GM-gap [\%]}: geometric-mean optimality gap at budget \(\tau\) for unsolved instances.
    \item \textbf{GM-nodes}: geometric-mean number of processed branch-and-bound nodes.
\end{itemize}

Each metric is reported under three evaluation modes:
\begin{itemize}
    \item \textbf{AC (Training)} — performance of the tuned configuration evaluated on the training instances used during configuration; \emph{Runs} reflects total evaluations on the training set.
    \item \textbf{AC (Testing)} — generalization performance of the tuned configuration on unseen test instances; \emph{Runs} is 0.
    \item \textbf{ISAC} — instance-specific configuration on the training set; \emph{Runs} is the mean evaluations per instance.
\end{itemize}

\begin{table*}[htp]
\centering
\small
\caption{\textbf{Results at \(\tau=300\,\mathrm{s}\)} (Curated whitelist). Columns: \emph{Runs}, \(\widehat{J}_S\), \emph{Solve [\%]}, \emph{GM-t [s]}, \emph{GM-gap [\%]}, \emph{GM-nodes}. Best per column in bold.}
\label{tab:main_results_300}
\begin{tabular}{lccccccc}
\hline
\textbf{Mode} & \textbf{Method} & \textbf{Runs} & \(\widehat{J}_S\) & \textbf{Solve [\%]} & \textbf{GM-t [s]} & \textbf{GM-gap [\%]} & \textbf{GM-nodes} \\
\hline
\multirow{4}{*}{AC (Training)}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\multirow{4}{*}{AC (Testing)}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\multirow{4}{*}{ISAC}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[htp]
\centering
\small
\caption{\textbf{Results at \(\tau=900\,\mathrm{s}\)} (Curated whitelist). Same layout and metrics as Table~\ref{tab:main_results_300}.}
\label{tab:main_results_900}
\begin{tabular}{lccccccc}
\hline
\textbf{Mode} & \textbf{Method} & \textbf{Runs} & \(\widehat{J}_S\) & \textbf{Solve [\%]} & \textbf{GM-t [s]} & \textbf{GM-gap [\%]} & \textbf{GM-nodes} \\
\hline
\multirow{4}{*}{AC (Training)}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\multirow{4}{*}{AC (Testing)}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\multirow{4}{*}{ISAC}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[htp]
\centering
\small
\caption{\textbf{Results at \(\tau=3600\,\mathrm{s}\)} (Curated whitelist). Same layout and metrics as Table~\ref{tab:main_results_300}.}
\label{tab:main_results_3600}
\begin{tabular}{lccccccc}
\hline
\textbf{Mode} & \textbf{Method} & \textbf{Runs} & \(\widehat{J}_S\) & \textbf{Solve [\%]} & \textbf{GM-t [s]} & \textbf{GM-gap [\%]} & \textbf{GM-nodes} \\
\hline
\multirow{4}{*}{AC (Training)}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\multirow{4}{*}{AC (Testing)}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\multirow{4}{*}{ISAC}
 & Default    & 0      & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SMAC       & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & RBFOpt     & [fill] & [fill] & [fill] & [fill] & [fill] & [fill] \\
 & SolverMind & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} \\
\hline
\end{tabular}
\end{table*}


\subsection{Ablation Study}
\label{sec:ablation}

We assess the impact of design choices on performance, sample efficiency, and overhead using a one-factor-at-a-time (controlled-variable) methodology: unless otherwise stated, each ablation varies a single factor relative to the defaults in Section~\ref{sec_results}. All ablations evaluate only \textit{SolverMind}; baseline optimizers (SMAC, RBFOpt) are omitted except where explicitly noted. To ensure comparability while keeping experiments lightweight, we fix the per-run time budget at \(\tau=900\,\mathrm{s}\) and evaluate all ablations on the CVRP case study’s training split.


\paragraph{Whitelist granularity (search-space design).}
We compare \emph{Full}, \emph{Curated}, and \emph{Minimal} whitelists to quantify the trade-off between search-space size and tuning efficacy. The Curated set aims to retain the most influential knobs while avoiding the brittleness and cost of the Full set. Results in Table~\ref{tab:ablation-regimes} show that Curated achieves the best balance: it closes a larger fraction of the VBC gap than Minimal while approaching Full’s best-case \(\widehat{J}_S(p)\) at substantially lower overhead. Based on these findings, the main experiments fix the whitelist to \emph{Curated}.

\begin{table}[t]
    \centering
    \small
    \caption{\textbf{Ablation: whitelist granularity.} Curated offers the best performance–efficiency trade-off. Brackets: 95\% BCa CIs; {\scriptsize\(\star\)}: significant vs.\ best non-SolverMind baseline.}
    \label{tab:ablation-regimes}
    \begin{tabular}{lcccc}
        \hline
        \textbf{Regime} & \(\widehat{J}_S(p)\)                  & solved@\(\tau\) [\%]                  & GM-time [s]     & Overhead [s]    \\
        \hline
        Full            & [fill]                                & [fill]                                & [fill]          & [fill]          \\
        Curated         & \textbf{[fill]}{\scriptsize\(\star\)} & \textbf{[fill]}{\scriptsize\(\star\)} & \textbf{[fill]} & \textbf{[fill]} \\
        Minimal         & [fill]                                & [fill]                                & [fill]          & \textbf{[fill]} \\
        \hline
    \end{tabular}
\end{table}

\paragraph{Edit and trial budgets (\(m\) and \(K\)).}
We sweep \(m\in\{1,3,5\}\) and \(K\in\{8,16,32,64\}\). Figure~\ref{fig:auc-k} plots area-under-improvement-curve (AUC) for \(\widehat{J}_S(p)\) against \(K\); Table~\ref{tab:ablation-mk} reports trials-to-\(5\%\) (smallest \(k\) within 5\% of final \(\widehat{J}_S\)), acceptance rates, and invalid-proposal rates. We observe diminishing returns beyond \(K=16\) and destabilization at \(m=5\) (higher invalid rates and regressions), while \(m=3\) consistently maximizes AUC under fixed budgets. We therefore adopt \(m=3,\,K=16\) as defaults.

\begin{table}[t]
    \centering
    \small
    \caption{\textbf{Ablation: edit cap \(m\) and trial cap \(K\).} Trials-to-5\% (lower is better), AUC (higher is better), and LLM proposal statistics.}
    \label{tab:ablation-mk}
    \begin{tabular}{lccc}
        \hline
        \textbf{Setting} & Trials-to-5\%   & AUC[\(1..K\)]   & Accept / Invalid [\%]    \\
        \hline
        \(m=1,\,K=16\)   & [fill]          & [fill]          & [fill] / [fill]          \\
        \(m=3,\,K=16\)   & \textbf{[fill]} & \textbf{[fill]} & \textbf{[fill]} / [fill] \\
        \(m=5,\,K=16\)   & [fill]          & [fill]          & [fill] / \textbf{[fill]} \\
        \(m=3,\,K=32\)   & [fill]          & [fill]          & [fill] / [fill]          \\
        \(m=3,\,K=64\)   & [fill]          & [fill]          & [fill] / [fill]          \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \fbox{\parbox{0.9\columnwidth}{\centering \vspace{1.8cm} \textit{AUC vs.\ trials \(K\)}; curves for \(m\in\{1,3,5\}\).\\[2mm] [figure placeholder] \vspace{1.8cm}}}
    \caption{\textbf{Sample efficiency.} Area under the \(\widehat{J}_S(p)\) improvement curve as a function of \(K\).}
    \label{fig:auc-k}
\end{figure}

\paragraph{Backbone LLM sensitivity.}
We compare several instruction-following models with tool-use support under identical prompts and schemas: \([{\rm LLM}\text{-}A]\), \([{\rm LLM}\text{-}B]\), \([{\rm LLM}\text{-}C]\) (closed-weight) and \([{\rm LLM}\text{-}D]\), \([{\rm LLM}\text{-}E]\) (open-weight) \emph{[fill specific models]}. Table~\ref{tab:ablation-llm} summarizes \(\widehat{J}_S\), solved@\(\tau\), acceptance rate, proposal latency, and estimated API cost. We find a clear accuracy–latency frontier: \([{\rm LLM}\text{-}A]\) achieves the best \(\widehat{J}_S\) with moderate latency; \([{\rm LLM}\text{-}D]\) offers lower cost at a small performance penalty. The main results adopt \([{\rm LLM}\text{-}A]\) \emph{[fill model]} for the best overall trade-off.

\begin{table}[t]
    \centering
    \small
    \caption{\textbf{Ablation: backbone LLM.} Same prompts, schemas, and budgets. Lower \(\widehat{J}_S\) is better.}
    \label{tab:ablation-llm}
    \begin{tabular}{lcccc}
        \hline
        \textbf{Model} & \(\widehat{J}_S(p)\) & solved@\(\tau\) [\%] & Latency [s]     & Cost [\$/run]   \\
        \hline
        LLM-A          & \textbf{[fill]}      & \textbf{[fill]}      & [fill]          & [fill]          \\
        LLM-B          & [fill]               & [fill]               & [fill]          & [fill]          \\
        LLM-C          & [fill]               & [fill]               & [fill]          & [fill]          \\
        LLM-D          & [fill]               & [fill]               & \textbf{[fill]} & \textbf{[fill]} \\
        LLM-E          & [fill]               & [fill]               & [fill]          & [fill]          \\
        \hline
    \end{tabular}
\end{table}



\paragraph{Scoring design: \(f_1\)–\(f_4\) forms and weights.}
We ablate the scoring components by testing (i) four \emph{extreme} variants that isolate a single criterion, (ii) an \emph{all-linear} variant, (iii) a \emph{no-caps} variant (removing the \(10^3\) upper bounds), and (iv) the adopted choice in Table~\ref{tab_f_functions}. Concretely:
\[
\begin{aligned}
\text{Status-only:}\;& f_1(x)=x,\ \ f_2\equiv f_3\equiv f_4\equiv 0;\\
\text{Runtime-only:}\;& f_2(x)=x,\ \ f_1\equiv f_3\equiv f_4\equiv 0;\\
\text{Gap-only:}\;& f_3(x)=x,\ \ f_1\equiv f_2\equiv f_4\equiv 0;\\
\text{Nodes-only:}\;& f_4(x)=x,\ \ f_1\equiv f_2\equiv f_3\equiv 0;\\
\text{All-linear:}\;& f_k(x)=x\ \ \text{for }k=1,\dots,4;\\
\text{No-caps:}\;& f_1(x)=10^3x,\ \ f_2(x)=x,\ \ f_3(x)=e^{x}-1,\ \ f_4(x)=10^{-3}x.
\end{aligned}
\]
We report \(\Delta \widehat{J}_S\) relative to the adopted design (negative is better), Kendall’s \(\tau\) against a practitioner-preference ranking (solved@\(\tau\)\(\succ\)runtime\(\succ\)gap), and tail robustness via the 95th-percentile \(\sigma\). As anticipated, single-criterion extremes distort behavior (e.g., \emph{runtime-only} ignores solution quality; \emph{gap-only} overspends time), \emph{all-linear} underweights hard-instance tails, and \emph{no-caps} is unstable on challenging cases. The adopted scheme best aligns with practitioner priorities while preserving stability on outliers.

\begin{table}[t]
\centering
\small
\caption{\textbf{Ablation: score components.} \(\Delta \widehat{J}_S\) (negative is better), rank agreement (Kendall’s \(\tau\)), and tail robustness (95th-percentile \(\sigma\)).}
\label{tab:ablation-f}
\begin{tabular}{lccc}
\hline
\textbf{Variant}                      & \(\Delta \widehat{J}_S\) & Kendall’s \(\tau\) & 95th-\% \(\sigma\) \\
\hline
Adopted (Table~\ref{tab_f_functions}) & \textbf{0}               & \textbf{[fill]}    & \textbf{[fill]}    \\
Status-only (\(f_1\) only)            & [fill]                   & [fill]             & [fill]             \\
Runtime-only (\(f_2\) only)           & [fill]                   & [fill]             & [fill]             \\
Gap-only (\(f_3\) only)               & [fill]                   & [fill]             & [fill]             \\
Nodes-only (\(f_4\) only)             & [fill]                   & [fill]             & [fill]             \\
All-linear (\(f_k(x)=x\))             & [fill]                   & [fill]             & [fill]             \\
No-caps (remove \(10^3\) bounds)      & [fill]                   & [fill]             & [fill]             \\
\hline
\end{tabular}
\end{table}


\section{Conclusion}

Your conclusion goes here.

\section*{Acknowledgements}

Your acknowledgements go here (remove for blind submission).

\section*{Impact Statement}


\clearpage

\bibliography{reference}
\bibliographystyle{icml2025}

\end{document}
